{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding entity classes in embeddings\n",
    "\n",
    "In this notebook we're going to use embeddings to find entity classes and how they correlate with other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import svm\n",
    "from keras.utils import get_file\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(12, 8)\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL    = 'GoogleNews-vectors-negative300.bin'\n",
    "data_loc = '/home/smithw/Downloads/deep_learning' # WS: files not backed up here\n",
    "zipped   = os.path.join(data_loc, MODEL + '.gz')  # WS mod\n",
    "unzipped = os.path.join(data_loc, MODEL)  # WS\n",
    "zipped, unzipped"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "path = get_file(MODEL + '.gz', 'https://s3.amazonaws.com/dl4j-distribution/%s.gz' % MODEL)\n",
    "unzipped = os.path.join('generated', MODEL)\n",
    "if not os.path.isfile(unzipped):\n",
    "    with open(unzipped, 'wb') as fout:\n",
    "        zcat = subprocess.Popen(['zcat'],\n",
    "                          stdin=open(path),\n",
    "                          stdout=fout\n",
    "                         )\n",
    "        zcat.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(unzipped, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Germany'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Annita_Kirsten'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we'll create a training set with countries and non countries and get a support vector machine to learn the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = list(csv.DictReader(open('data/countries.csv')))\n",
    "len(countries), countries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examples of country names\n",
    "positive = [x['name'] for x in random.sample(countries, 40)]\n",
    "positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exampls of not-country names\n",
    "#negative = random.sample(model.vocab.keys(), 5000)  # WS vocab is OBE\n",
    "negative = random.sample(model.index_to_key, 5000)  # WS this works\n",
    "negative[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.key_to_index), len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled = [(p, 1) for p in positive] + [(n, 0) for n in negative]\n",
    "random.shuffle(labelled)\n",
    "X = np.asarray([model[w] for w, l in labelled])\n",
    "y = np.asarray([l for w, l in labelled])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FRACTION = 0.3\n",
    "cut_off = int(TRAINING_FRACTION * len(labelled))\n",
    "clf     = svm.SVC(kernel='linear')\n",
    "clf.fit(X[:cut_off], y[:cut_off])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict(X[cut_off:])\n",
    "\n",
    "missed = [country for (pred, truth, country) in \n",
    " zip(res, y[cut_off:], labelled[cut_off:]) if pred != truth]\n",
    "\n",
    "100 - 100 * float(len(missed)) / len(res), missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.mean(), X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: if too many vectors are taken, RAM may fill up, with the word2vec dbase already\n",
    "# taking up a lot of RAM; the full 3000000 is too large\n",
    "all_predictions = clf.predict(model.vectors[:1000000]) # 1e6 takes 30s to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for word, pred in zip(model.index_to_key, all_predictions):  # WS index_to_key replaces index2word\n",
    "    if pred:\n",
    "        res.append(word)  # WS turned off break: see how many hits there are\n",
    "        #if len(res) == 150:\n",
    "        #    break\n",
    "random.sample(res, 20) # can see the false alarms mixed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_idx = {country['name']: idx for idx, country in enumerate(countries)}\n",
    "country_vecs = np.asarray([model[c['name']] for c in countries])\n",
    "country_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to see what is similar to Canada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.dot(country_vecs, country_vecs[country_to_idx['Canada']])\n",
    "for idx in reversed(np.argsort(dists)[-10:]):\n",
    "    print(countries[idx]['name'], dists[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking countries for a specific term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_countries(term, topn=10, field='name'):\n",
    "    if not term in model:\n",
    "        return []\n",
    "    vec = model[term]\n",
    "    dists = np.dot(country_vecs, vec)\n",
    "    return [(countries[idx][field], float(dists[idx])) \n",
    "            for idx in reversed(np.argsort(dists)[-topn:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_countries('cricket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize this on a world map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot some maps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_term(term):\n",
    "    d = {k.upper(): v for k, v in rank_countries(term, topn=0, field='cc3')}\n",
    "    world[term] = world['iso_a3'].map(d)\n",
    "    world[term] /= world[term].max()\n",
    "    world.dropna().plot(term, cmap='OrRd')\n",
    "\n",
    "map_term('coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_term('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_term('China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_term('vodka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
